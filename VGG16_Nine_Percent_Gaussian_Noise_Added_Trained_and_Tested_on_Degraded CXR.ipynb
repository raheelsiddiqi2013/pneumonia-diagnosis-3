{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Diagnosis based on Degraded Chest X-ray Images \n",
    "The author of this notebook is **Dr Raheel Siddiqi** on *03-08-2020*. He is a *Senior Assistant Professor* at Bahria University, Karachi Campus, Pakistan. His research interests include *Medical Imaging*, *Computer Vision* and *Deep Learning*.\n",
    "\n",
    "The objective of the experiment (presented in this notebook) is to evaluate the effectiveness of **VGG16** pre-trained model when employed to diagnose Pneumonia (based on degraded CXR images - 9% Gaussian Noise Added). The diagnosis process comprises of classifying Chest X-ray images into one of the two classes: (a) Normal, (b) Pneumonia.\n",
    "\n",
    "The dataset is downloded from [kaggle.com](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia). The dataset contains 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal). The images are pre-processed and gaussian noise is added to each chest X-ray image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow and Keras version used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  1.13.1\n",
      "Keras Version:  2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "\n",
    "print('Tensorflow Version: ', tf.__version__)\n",
    "print('Keras Version: ', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Raheel\\Anaconda3\\envs\\EnvName\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.applications import VGG16\n",
    "\n",
    "conv_base=VGG16(weights='imagenet',include_top=False,input_shape=(150,150,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=1e-4),metrics=['categorical_accuracy'])\n",
    "filepath=\"VGG16_Nine_Percent_Gaussian_Noise_Added_Trained_and_Tested_on_Degraded_CXR.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 18,910,530\n",
      "Trainable params: 18,910,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Training and Validation Data for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 150\n",
    "image_width = 150\n",
    "batch_size = 8\n",
    "no_of_epochs  = 100\n",
    "number_of_training_samples=5216\n",
    "number_of_validation_samples=16\n",
    "number_of_test_samples=624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir='D:\\\\TensorFlow Programs\\\\Diagnosis of Pneumonia based on Chest X-Rays\\\\new experiments_from 24_08_2019\\\\Pediatric Chest X_Rays_nine_percent_Gaussian_Noise_Added\\\\train'\n",
    "validation_dir='D:\\\\TensorFlow Programs\\\\Diagnosis of Pneumonia based on Chest X-Rays\\\\new experiments_from 24_08_2019\\\\Pediatric Chest X_Rays_nine_percent_Gaussian_Noise_Added\\\\val'\n",
    "test_dir='D:\\\\TensorFlow Programs\\\\Diagnosis of Pneumonia based on Chest X-Rays\\\\new experiments_from 24_08_2019\\\\Pediatric Chest X_Rays_nine_percent_Gaussian_Noise_Added\\\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2\n",
    "                                   )\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)  \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(train_dir,target_size=(image_width, image_height),batch_size=batch_size)\n",
    "validation_set = validation_datagen.flow_from_directory(validation_dir,target_size=(image_width, image_height),batch_size=batch_size,shuffle=False)\n",
    "test_set = test_datagen.flow_from_directory(test_dir,target_size=(image_width, image_height),batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Raheel\\Anaconda3\\envs\\EnvName\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 290ms/step - loss: 0.4840 - categorical_accuracy: 0.6875\n",
      "652/652 [==============================] - 224s 344ms/step - loss: 0.4168 - categorical_accuracy: 0.8037 - val_loss: 0.4840 - val_categorical_accuracy: 0.6875\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 325ms/step - loss: 0.5502 - categorical_accuracy: 0.7500\n",
      "652/652 [==============================] - 138s 212ms/step - loss: 0.3454 - categorical_accuracy: 0.8447 - val_loss: 0.5502 - val_categorical_accuracy: 0.7500\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.7948 - categorical_accuracy: 0.5000\n",
      "652/652 [==============================] - 129s 197ms/step - loss: 0.2990 - categorical_accuracy: 0.8631 - val_loss: 0.7948 - val_categorical_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.7577 - categorical_accuracy: 0.6250\n",
      "652/652 [==============================] - 127s 194ms/step - loss: 0.2448 - categorical_accuracy: 0.8982 - val_loss: 0.7577 - val_categorical_accuracy: 0.6250\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.9558 - categorical_accuracy: 0.6250\n",
      "652/652 [==============================] - 127s 194ms/step - loss: 0.1997 - categorical_accuracy: 0.9176 - val_loss: 0.9558 - val_categorical_accuracy: 0.6250\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.5919 - categorical_accuracy: 0.8125\n",
      "652/652 [==============================] - 127s 194ms/step - loss: 0.1772 - categorical_accuracy: 0.9271 - val_loss: 0.5919 - val_categorical_accuracy: 0.8125\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.3811 - categorical_accuracy: 0.8125\n",
      "652/652 [==============================] - 127s 194ms/step - loss: 0.1596 - categorical_accuracy: 0.9352 - val_loss: 0.3811 - val_categorical_accuracy: 0.8125\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.3115 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 127s 195ms/step - loss: 0.1709 - categorical_accuracy: 0.9337 - val_loss: 0.3115 - val_categorical_accuracy: 0.8750\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.3611 - categorical_accuracy: 0.8125\n",
      "652/652 [==============================] - 126s 193ms/step - loss: 0.1479 - categorical_accuracy: 0.9438 - val_loss: 0.3611 - val_categorical_accuracy: 0.8125\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.5950 - categorical_accuracy: 0.8125\n",
      "652/652 [==============================] - 126s 194ms/step - loss: 0.1550 - categorical_accuracy: 0.9398 - val_loss: 0.5950 - val_categorical_accuracy: 0.8125\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.2498 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 127s 195ms/step - loss: 0.1253 - categorical_accuracy: 0.9536 - val_loss: 0.2498 - val_categorical_accuracy: 0.9375\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.3377 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 126s 193ms/step - loss: 0.1252 - categorical_accuracy: 0.9530 - val_loss: 0.3377 - val_categorical_accuracy: 0.8750\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.4965 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 126s 194ms/step - loss: 0.1204 - categorical_accuracy: 0.9528 - val_loss: 0.4965 - val_categorical_accuracy: 0.8750\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 0.2834 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 127s 194ms/step - loss: 0.1170 - categorical_accuracy: 0.9557 - val_loss: 0.2834 - val_categorical_accuracy: 0.8750\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 174ms/step - loss: 0.3743 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 221s 339ms/step - loss: 0.1062 - categorical_accuracy: 0.9601 - val_loss: 0.3743 - val_categorical_accuracy: 0.8750\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.1945 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 158s 242ms/step - loss: 0.1043 - categorical_accuracy: 0.9636 - val_loss: 0.1945 - val_categorical_accuracy: 0.8750\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.3270 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 157s 241ms/step - loss: 0.1194 - categorical_accuracy: 0.9538 - val_loss: 0.3270 - val_categorical_accuracy: 0.9375\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.1575 - categorical_accuracy: 1.0000\n",
      "652/652 [==============================] - 209s 321ms/step - loss: 0.1121 - categorical_accuracy: 0.9590 - val_loss: 0.1575 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 1.1532 - categorical_accuracy: 0.6875\n",
      "652/652 [==============================] - 145s 222ms/step - loss: 0.1318 - categorical_accuracy: 0.9519 - val_loss: 1.1532 - val_categorical_accuracy: 0.6875\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.4174 - categorical_accuracy: 0.8125\n",
      "652/652 [==============================] - 136s 208ms/step - loss: 0.1020 - categorical_accuracy: 0.9638 - val_loss: 0.4174 - val_categorical_accuracy: 0.8125\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.9463 - categorical_accuracy: 0.6875\n",
      "652/652 [==============================] - 136s 208ms/step - loss: 0.0903 - categorical_accuracy: 0.9670 - val_loss: 0.9463 - val_categorical_accuracy: 0.6875\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.4210 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0889 - categorical_accuracy: 0.9641 - val_loss: 0.4210 - val_categorical_accuracy: 0.8750\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.1830 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0878 - categorical_accuracy: 0.9672 - val_loss: 0.1830 - val_categorical_accuracy: 0.8750\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.0769 - categorical_accuracy: 1.0000\n",
      "652/652 [==============================] - 135s 206ms/step - loss: 0.0949 - categorical_accuracy: 0.9636 - val_loss: 0.0769 - val_categorical_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 175ms/step - loss: 0.3127 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.1112 - categorical_accuracy: 0.9670 - val_loss: 0.3127 - val_categorical_accuracy: 0.8750\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 187ms/step - loss: 0.1789 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 206ms/step - loss: 0.1119 - categorical_accuracy: 0.9649 - val_loss: 0.1789 - val_categorical_accuracy: 0.8750\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.3051 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 135s 208ms/step - loss: 0.0814 - categorical_accuracy: 0.9672 - val_loss: 0.3051 - val_categorical_accuracy: 0.8750\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.2285 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0968 - categorical_accuracy: 0.9678 - val_loss: 0.2285 - val_categorical_accuracy: 0.8750\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.1324 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0867 - categorical_accuracy: 0.9668 - val_loss: 0.1324 - val_categorical_accuracy: 0.9375\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 156ms/step - loss: 0.5120 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 206ms/step - loss: 0.0765 - categorical_accuracy: 0.9724 - val_loss: 0.5120 - val_categorical_accuracy: 0.8750\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.5545 - categorical_accuracy: 0.8125\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0730 - categorical_accuracy: 0.9699 - val_loss: 0.5545 - val_categorical_accuracy: 0.8125\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.1785 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 206ms/step - loss: 0.0732 - categorical_accuracy: 0.9732 - val_loss: 0.1785 - val_categorical_accuracy: 0.9375\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.3233 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 206ms/step - loss: 0.0782 - categorical_accuracy: 0.9701 - val_loss: 0.3233 - val_categorical_accuracy: 0.9375\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.4749 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0740 - categorical_accuracy: 0.9726 - val_loss: 0.4749 - val_categorical_accuracy: 0.8750\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.5455 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0718 - categorical_accuracy: 0.9732 - val_loss: 0.5455 - val_categorical_accuracy: 0.8750\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 0.8521 - categorical_accuracy: 0.8125\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0714 - categorical_accuracy: 0.9751 - val_loss: 0.8521 - val_categorical_accuracy: 0.8125\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.9245 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0683 - categorical_accuracy: 0.9755 - val_loss: 0.9245 - val_categorical_accuracy: 0.8750\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.4536 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.1941 - categorical_accuracy: 0.9548 - val_loss: 0.4536 - val_categorical_accuracy: 0.9375\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.3007 - categorical_accuracy: 0.8125\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.1228 - categorical_accuracy: 0.9538 - val_loss: 0.3007 - val_categorical_accuracy: 0.8125\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.7640 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0785 - categorical_accuracy: 0.9707 - val_loss: 0.7640 - val_categorical_accuracy: 0.8750\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.3923 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 133s 205ms/step - loss: 0.0661 - categorical_accuracy: 0.9745 - val_loss: 0.3923 - val_categorical_accuracy: 0.8750\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 175ms/step - loss: 0.1959 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0621 - categorical_accuracy: 0.9778 - val_loss: 0.1959 - val_categorical_accuracy: 0.9375\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.2193 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0871 - categorical_accuracy: 0.9697 - val_loss: 0.2193 - val_categorical_accuracy: 0.8750\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.2391 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0748 - categorical_accuracy: 0.9747 - val_loss: 0.2391 - val_categorical_accuracy: 0.9375\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.3004 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0603 - categorical_accuracy: 0.9755 - val_loss: 0.3004 - val_categorical_accuracy: 0.9375\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.8659 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0932 - categorical_accuracy: 0.9697 - val_loss: 0.8659 - val_categorical_accuracy: 0.9375\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.1947 - categorical_accuracy: 0.8125\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.1151 - categorical_accuracy: 0.9655 - val_loss: 0.1947 - val_categorical_accuracy: 0.8125\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.1280 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.1170 - categorical_accuracy: 0.9626 - val_loss: 0.1280 - val_categorical_accuracy: 0.9375\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.1646 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 206ms/step - loss: 0.0831 - categorical_accuracy: 0.9699 - val_loss: 0.1646 - val_categorical_accuracy: 0.8750\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.2272 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0662 - categorical_accuracy: 0.9770 - val_loss: 0.2272 - val_categorical_accuracy: 0.8750\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.2050 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 206ms/step - loss: 0.0659 - categorical_accuracy: 0.9743 - val_loss: 0.2050 - val_categorical_accuracy: 0.9375\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.5791 - categorical_accuracy: 0.7500\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0644 - categorical_accuracy: 0.9762 - val_loss: 0.5791 - val_categorical_accuracy: 0.7500\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.1944 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0583 - categorical_accuracy: 0.9791 - val_loss: 0.1944 - val_categorical_accuracy: 0.8750\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.2056 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0525 - categorical_accuracy: 0.9812 - val_loss: 0.2056 - val_categorical_accuracy: 0.8750\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.1081 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0533 - categorical_accuracy: 0.9803 - val_loss: 0.1081 - val_categorical_accuracy: 0.9375\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.2567 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 206ms/step - loss: 0.0662 - categorical_accuracy: 0.9781 - val_loss: 0.2567 - val_categorical_accuracy: 0.8750\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.3582 - categorical_accuracy: 0.8750\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0587 - categorical_accuracy: 0.9781 - val_loss: 0.3582 - val_categorical_accuracy: 0.8750\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.1602 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0556 - categorical_accuracy: 0.9791 - val_loss: 0.1602 - val_categorical_accuracy: 0.9375\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.1703 - categorical_accuracy: 0.9375\n",
      "652/652 [==============================] - 134s 205ms/step - loss: 0.0683 - categorical_accuracy: 0.9768 - val_loss: 0.1703 - val_categorical_accuracy: 0.9375\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47/652 [=>............................] - ETA: 2:03 - loss: 0.0413 - categorical_accuracy: 0.9867"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "history = model.fit_generator(\n",
    "      training_set,\n",
    "      steps_per_epoch=math.ceil(number_of_training_samples//batch_size),\n",
    "      epochs=no_of_epochs,\n",
    "      callbacks=callbacks_list,\n",
    "      validation_data=validation_set,\n",
    "      validation_steps=math.ceil(number_of_validation_samples//batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc=history.history['categorical_accuracy']\n",
    "val_acc=history.history['val_categorical_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(1,len(acc)+1)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "best_model = load_model('VGG16_Nine_Percent_Gaussian_Noise_Added_Trained_and_Tested_on_Degraded_CXR.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_test=int(number_of_test_samples/batch_size)\n",
    "result = best_model.evaluate_generator(test_set, steps=steps_test,verbose=1)\n",
    "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Precision, Specificity and Sensitivity (Recall) values for the model\n",
    "*Precision-Recall* is a useful measure of success of prediction when the classes are very imbalanced. High *precision* relates to a low false positive rate, and high *recall* relates to a low false negative rate.\n",
    "\n",
    "Precision is defined as the number of true positives over the number of true positives plus the number of false positives. Recall is defined as the number of true positives over the number of true positives plus the number of false negatives. Please see the [link](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) for details.\n",
    "\n",
    "Another way of assessing a binary classification model is to compute the *sensitivity* (also called the true positive rate or the recall) and *specificity* (also called the true negative rate) of the model. *Sensitivity* indicates the percentage of sick people who are correctly identified as having the condition. *Specificity* indicates the percentage of healthy people who are correctly identified as not having the condition.\n",
    "\n",
    "But first we prepare our test data, so that later we can draw the *confusion matrix* as well as calculate *precision*, *specificity* and *sensitivity (recall)* of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing test data\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "import cv2\n",
    "\n",
    "normal_cases_dir = Path('D:\\\\TensorFlow Programs\\\\Diagnosis of Pneumonia based on Chest X-Rays\\\\new experiments_from 24_08_2019\\\\Pediatric Chest X_Rays_nine_percent_Gaussian_Noise_Added\\\\test\\\\NORMAL')\n",
    "pneumonia_cases_dir = Path('D:\\\\TensorFlow Programs\\\\Diagnosis of Pneumonia based on Chest X-Rays\\\\new experiments_from 24_08_2019\\\\Pediatric Chest X_Rays_nine_percent_Gaussian_Noise_Added\\\\test\\\\PNEUMONIA')\n",
    "\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for img in normal_cases:\n",
    "    img = cv2.imread(str(img))\n",
    "    img = cv2.resize(img, (image_width,image_height))\n",
    "    if img.shape[2] ==1:\n",
    "        img = np.dstack([img, img, img])\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = [0]\n",
    "    test_data.append(img)\n",
    "    test_labels.append(label)\n",
    "                      \n",
    "for img in pneumonia_cases:\n",
    "    img = cv2.imread(str(img))\n",
    "    img = cv2.resize(img, (image_width,image_height))\n",
    "    if img.shape[2] ==1:\n",
    "        img = np.dstack([img, img, img])\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = [1]\n",
    "    test_data.append(img)\n",
    "    test_labels.append(label)\n",
    "    \n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "test_labels_one_hot_encoding=to_categorical(test_labels)\n",
    "\n",
    "print(\"Total number of test examples: \", test_data.shape)\n",
    "print(\"Total number of labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test dataset\n",
    "\n",
    "test_loss, test_score = best_model.evaluate(test_data, test_labels_one_hot_encoding, batch_size=16)\n",
    "print(\"Loss on test set: \", test_loss)\n",
    "print(\"Accuracy on test set: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds = best_model.predict(test_data, batch_size=16,verbose=1)\n",
    "preds=np.around(preds)\n",
    "orig_test_labels=test_labels\n",
    "predicted_labels=np.argmax(preds,axis=1)\n",
    "predicted_labels=np.reshape(predicted_labels,(624,1))\n",
    "print(predicted_labels.shape)\n",
    "print(orig_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Get the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "cm  = confusion_matrix(orig_test_labels, predicted_labels)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision and Recall\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "specificity=tn/(tn+fp)\n",
    "\n",
    "print(\"Sensitivity (Recall) of the model is {:.3f}\".format(recall))\n",
    "print(\"Specificity of the model is {:.3f}\".format(specificity))\n",
    "print(\"Precision of the model is {:.3f}\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!!! So, this model has a `%` sensitivity (recall), `%` specificity and `%` precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve and AUC for the model\n",
    "**Receiver operating characteristic** curve (a.k.a ROC) is a graphic plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The critical point here is \"binary classifier\" and \"varying threshold\". **Area Under the Curve** (a.k.a. AUC) is the percentage of the area that is under this ROC curve, ranging between 0 and 1.\n",
    "ROC is a great way to visualize the performance of a binary classifier, and AUC is one single number to summarize a classifier's performance. The higher the AUC is, the better the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "probs = best_model.predict_proba(test_data)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(test_labels.ravel(), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc_var = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='VGG16 based model (area = {:.3f})'.format(auc_var))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(test_labels.ravel(), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_var2 = auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(recall, precision, label='VGG16 based model (area = {:.3f})'.format(auc_var2))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Precision Score: {:.3f}'.format(average_precision_score(test_labels.ravel(), preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score: {:.3f}\".format(f1_score(test_labels.ravel(),predicted_labels.ravel())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
